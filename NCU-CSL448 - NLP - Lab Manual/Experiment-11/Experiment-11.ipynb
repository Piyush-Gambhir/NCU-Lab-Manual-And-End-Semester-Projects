{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Experiment-11**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: \n",
    "Write a program to compute the similarity between any two sentences of text using different metrics for analyzing textual similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mainp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\mainp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: click in c:\\users\\mainp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mainp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mainp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mainp\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mainp\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mainp\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 30.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 27.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 26.4 MB/s eta 0:00:00\n",
      "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 4.5/44.5 MB 20.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.7/44.5 MB 23.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.5/44.5 MB 24.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 23.1/44.5 MB 27.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.5/44.5 MB 29.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.4/44.5 MB 31.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.3/44.5 MB 31.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 28.9 MB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Download required NLTK data files\n",
    "nltk.download('punkt', download_dir=\"C:/nltk_data\")\n",
    "nltk.download('stopwords', download_dir=\"C:/nltk_data\")\n",
    "nltk.download('wordnet', download_dir=\"C:/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: The quick brown fox jumps over the lazy dog.\n",
      "Sentence 2: A fast brown animal leaps over a sleeping canine.\n"
     ]
    }
   ],
   "source": [
    "# Define two sentences for similarity analysis\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sentence2 = \"A fast brown animal leaps over a sleeping canine.\"\n",
    "\n",
    "print(\"Sentence 1:\", sentence1)\n",
    "print(\"Sentence 2:\", sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens for Sentence 1: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "Processed Tokens for Sentence 2: ['fast', 'brown', 'animal', 'leaps', 'sleeping', 'canine']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess both sentences\n",
    "tokens1 = preprocess_text(sentence1)\n",
    "tokens2 = preprocess_text(sentence2)\n",
    "\n",
    "print(\"Processed Tokens for Sentence 1:\", tokens1)\n",
    "print(\"Processed Tokens for Sentence 2:\", tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (TF-IDF): 0.13049436152984825\n"
     ]
    }
   ],
   "source": [
    "# Combine the sentences into a corpus\n",
    "corpus = [sentence1, sentence2]\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "print(\"Cosine Similarity (TF-IDF):\", cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "# Define a function to compute Jaccard Similarity\n",
    "def jaccard_similarity(tokens1, tokens2):\n",
    "    set1 = set(tokens1)\n",
    "    set2 = set(tokens2)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "# Compute Jaccard Similarity\n",
    "jaccard_sim = jaccard_similarity(tokens1, tokens2)\n",
    "print(\"Jaccard Similarity:\", jaccard_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance: 32\n"
     ]
    }
   ],
   "source": [
    "# Define a function to compute Levenshtein Distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "# Compute Levenshtein Distance\n",
    "levenshtein_dist = levenshtein_distance(sentence1, sentence2)\n",
    "print(\"Levenshtein Distance:\", levenshtein_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet-Based Similarity: 0.10795859026357747\n"
     ]
    }
   ],
   "source": [
    "# Define a function to compute WordNet similarity\n",
    "def wordnet_similarity(tokens1, tokens2):\n",
    "    synsets1 = [wn.synsets(word)[0] for word in tokens1 if wn.synsets(word)]\n",
    "    synsets2 = [wn.synsets(word)[0] for word in tokens2 if wn.synsets(word)]\n",
    "\n",
    "    score = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            sim = synset1.path_similarity(synset2)\n",
    "            if sim is not None:\n",
    "                score += sim\n",
    "                count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return score / count\n",
    "\n",
    "# Compute WordNet Similarity\n",
    "wordnet_sim = wordnet_similarity(tokens1, tokens2)\n",
    "print(\"WordNet-Based Similarity:\", wordnet_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Sentence Similarity Metrics:\n",
      "Cosine Similarity (TF-IDF): 0.1305\n",
      "Jaccard Similarity: 0.0909\n",
      "Levenshtein Distance: 32\n",
      "WordNet-Based Similarity: 0.1080\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSummary of Sentence Similarity Metrics:\")\n",
    "print(f\"Cosine Similarity (TF-IDF): {cosine_sim:.4f}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_sim:.4f}\")\n",
    "print(f\"Levenshtein Distance: {levenshtein_dist}\")\n",
    "print(f\"WordNet-Based Similarity: {wordnet_sim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
